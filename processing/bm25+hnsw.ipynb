{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 228kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 43.6kB/s]\n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.58MB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 13.1MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:02<00:00, 42.3MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 3.92kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.0kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 6.56MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 116kB/s]\n",
      "train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 2.69MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 22.4MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 333kB/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import subprocess\n",
    "import torch\n",
    "from getPassageFromPID import DocContentManager\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keybert import KeyBERT\n",
    "import sys\n",
    "import hnswlib\n",
    "sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "dimension = 384 \n",
    "index = hnswlib.Index(space='cosine', dim=dimension)\n",
    "index.load_index('hnsw_index_all.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DocContentManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeyBERT - Query Expansion and Keyword Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandQueryByKeyBert(query, passage_list, k=5):\n",
    "    all_passages = \" \".join(passage_list)\n",
    "    keywords = kw_model.extract_keywords(all_passages, stop_words=query.split() ,top_n=k)\n",
    "    new_user_query =  \" \".join([term for term, _ in keywords]) + query\n",
    "    return new_user_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to extract keywords from a single query\n",
    "def extract_keywords_from_query(query):\n",
    "    # Tokenize the query\n",
    "    words = word_tokenize(query)\n",
    "    # Convert to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKeywords(query):\n",
    "    k = len(query.split()) - 1\n",
    "    keywords = kw_model.extract_keywords(query, top_n=k)\n",
    "    new_user_query =  \" \".join([term for term, _ in keywords])\n",
    "    return new_user_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeBM25(query):\n",
    "\n",
    "    relevant_docs = []\n",
    "\n",
    "    query = extract_keywords_from_query(query)\n",
    "    query = \" \".join(query) \n",
    "    while True:\n",
    "        cpp_executable_path = './bm25' \n",
    "        command = [cpp_executable_path, query]\n",
    "        process = subprocess.run(command, capture_output=True, text=True)\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            output_lines = process.stdout.strip().split('\\n')\n",
    "\n",
    "            # Process each line of output\n",
    "            for line in output_lines:\n",
    "                if line == \"Word not found in index map.\" or line == \"\":\n",
    "                    # print(\"New word - not found in corpus\")\n",
    "                    continue\n",
    "                else:\n",
    "                    pid = int(line)\n",
    "                    if pid < 7999999: \n",
    "                        relevant_docs.append(pid)\n",
    "        else:\n",
    "            print(\"Error:\", process.stderr)\n",
    "        if len(relevant_docs) == 0 and len(query)>0:\n",
    "            query = findKeywords(query)\n",
    "        else:\n",
    "            return relevant_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(inp_question):\n",
    "    return sentence_transformer_model.encode(inp_question, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hnsw(input_question, k=100):\n",
    "    final_passages = []\n",
    "    # print(input_question, \" in semantic_search()\")\n",
    "    relevant_docs = executeBM25(input_question)\n",
    "    # print(relevant_docs)\n",
    "    relevant_embeddings = []\n",
    "    for doc in relevant_docs:\n",
    "        relevant_embeddings.append(get_embedding(d.fetchPassageContent(doc)))\n",
    "    labels, distances = index.knn_query(relevant_embeddings, k=k)\n",
    "    print(labels)\n",
    "    return final_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n",
      "Error: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhnsw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manimal cages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mhnsw\u001b[1;34m(input_question, k)\u001b[0m\n\u001b[0;32m      2\u001b[0m final_passages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(input_question, \" in semantic_search()\")\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mexecuteBM25\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(relevant_docs)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m relevant_embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mexecuteBM25\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, process\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(relevant_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mfindKeywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m relevant_docs\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mfindKeywords\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindKeywords\u001b[39m(query):\n\u001b[0;32m      2\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(query\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[43mkw_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     new_user_query \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([term \u001b[38;5;28;01mfor\u001b[39;00m term, _ \u001b[38;5;129;01min\u001b[39;00m keywords])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_user_query\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keybert\\_model.py:188\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     doc_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(words)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py:62\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = hnsw(\"animal cages\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
